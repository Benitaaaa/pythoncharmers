{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91856fc8-1d48-4530-9954-0050bb5e7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "#!pip install pycountry #this is to standardize all country names\n",
    "#!pip install pandas openpyxl requests beautifulsoup4 spacy transformers pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89262263-2c26-42d8-8b79-0cce9272b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline\n",
    "from textblob import TextBlob\n",
    "from pyvis.network import Network\n",
    "import pycountry\n",
    "net = Network(notebook=True)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e95d270-8921-42e4-a185-48cf808e3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f489a93-ead3-4778-b2ab-848eed26f4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://edition.cnn.com/2023/09/29/business/st...</td>\n",
       "      <td>Starbucks violated federal labor law when it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/su-w...</td>\n",
       "      <td>The first suspect to plead guilty in Singapore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://edition.cnn.com/2023/05/22/tech/meta-f...</td>\n",
       "      <td>Meta has been fined a record-breaking €1.2 bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/bill...</td>\n",
       "      <td>SINGAPORE: A 45-year-old man linked to Singapo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://edition.cnn.com/2024/03/05/politics/li...</td>\n",
       "      <td>The Department of Education imposed a record $...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://edition.cnn.com/2023/09/29/business/st...   \n",
       "1  https://www.channelnewsasia.com/singapore/su-w...   \n",
       "2  https://edition.cnn.com/2023/05/22/tech/meta-f...   \n",
       "3  https://www.channelnewsasia.com/singapore/bill...   \n",
       "4  https://edition.cnn.com/2024/03/05/politics/li...   \n",
       "\n",
       "                                                Text  \n",
       "0  Starbucks violated federal labor law when it i...  \n",
       "1  The first suspect to plead guilty in Singapore...  \n",
       "2  Meta has been fined a record-breaking €1.2 bil...  \n",
       "3  SINGAPORE: A 45-year-old man linked to Singapo...  \n",
       "4  The Department of Education imposed a record $...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the uploaded CSV file\n",
    "file_path = \"/Users/benitaleonardi/Documents/news_excerpts_parsed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff793414-31e7-44a7-8e01-6c2bc38257e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://edition.cnn.com/2023/09/29/business/starbucks-union-wages/index.html\n",
      "Scraping: https://www.channelnewsasia.com/singapore/su-wenqiang-pleads-guilty-billion-dollar-money-laundering-convicted-4234731\n",
      "Scraping: https://edition.cnn.com/2023/05/22/tech/meta-facebook-data-privacy-eu-fine/index.html\n",
      "Scraping: https://www.channelnewsasia.com/singapore/billion-dollar-money-laundering-case-zhang-ruijin-sentenced-15-months-jail-4302416\n",
      "Scraping: https://edition.cnn.com/2024/03/05/politics/liberty-university-fined-campus-safety/index.html\n",
      "Scraping: https://www.euronews.com/2024/02/23/judge-convicts-former-austrian-chancellor-sebastian-kurz\n",
      "Scraping: https://edition.cnn.com/2022/07/21/economy/china-fines-didi-data-law-violation-intl-hnk/index.html\n",
      "Scraping: https://www.brusselstimes.com/justice-belgium/1011990/two-alleged-leaders-of-vast-drug-trafficking-operation-looking-at-20-years-in-prison\n",
      "Scraping: https://www.expats.cz/czech-news/article/former-czech-mp-gets-three-year-jail-term-for-multiple-charges-of-rape\n",
      "Scraping: https://www.thelocal.dk/20240311/british-trader-on-trial-in-denmark-for-massive-fraud\n",
      "Failed to fetch https://www.thelocal.dk/20240311/british-trader-on-trial-in-denmark-for-massive-fraud: 403 Client Error: Forbidden for url: https://www.thelocal.dk/20240311/british-trader-on-trial-in-denmark-for-massive-fraud\n",
      "country_network_2.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import pycountry\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline\n",
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Hugging Face Transformers Sentiment Model\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "class CountrySentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the text analyzer with NLP and data structures.\"\"\"\n",
    "        self.nlp = nlp  # Use loaded SpaCy model\n",
    "        self.entities = defaultdict(set)\n",
    "        self.relationships = []\n",
    "\n",
    "    def standardize_country_name(self, country):\n",
    "        \"\"\"Attempt to standardize country names using pycountry.\"\"\"\n",
    "        try:\n",
    "            return pycountry.countries.lookup(country).name\n",
    "        except LookupError:\n",
    "            return None  # Ignore non-country entities\n",
    "\n",
    "    def get_sentiment(self, sentence):\n",
    "        \"\"\"Analyze sentiment of a sentence using Transformers.\"\"\"\n",
    "        result = sentiment_pipeline(sentence)\n",
    "        sentiment_label = result[0]['label']  # 'POSITIVE' or 'NEGATIVE'\n",
    "        confidence = result[0]['score']\n",
    "\n",
    "        # If confidence is low, classify as NEUTRAL\n",
    "        if confidence < 0.7:\n",
    "            return 'NEUTRAL'\n",
    "        return sentiment_label\n",
    "\n",
    "    def extract_entities_and_relationships(self, text, source=None):\n",
    "        \"\"\"Extract country entities (GPE) and relationships from text.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        country_entities = set()\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"GPE\":\n",
    "                country_name = self.standardize_country_name(ent.text)\n",
    "                if country_name:  # Only add valid countries\n",
    "                    country_entities.add((country_name, source))\n",
    "\n",
    "        # Store valid countries\n",
    "        for country in country_entities:\n",
    "            self.entities['GPE'].add(country)\n",
    "\n",
    "        # Create relationships between countries in the same sentence\n",
    "        for sent in doc.sents:\n",
    "            sent_doc = self.nlp(sent.text)\n",
    "            entities_in_sent = [self.standardize_country_name(e.text) for e in sent_doc.ents if e.label_ == \"GPE\"]\n",
    "            entities_in_sent = [e for e in entities_in_sent if e]  # Remove None values\n",
    "\n",
    "            if len(entities_in_sent) >= 2:\n",
    "                sentiment = self.get_sentiment(sent.text)  # Get sentiment of the sentence\n",
    "                for i in range(len(entities_in_sent) - 1):\n",
    "                    self.relationships.append({\n",
    "                        'source': entities_in_sent[i],\n",
    "                        'target': entities_in_sent[i + 1],\n",
    "                        'sentence': sent.text,\n",
    "                        'sentiment': sentiment\n",
    "                    })\n",
    "\n",
    "    def scrape_webpage(self, url):\n",
    "        \"\"\"Scrape webpage content from a given URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            paragraphs = [p.text for p in soup.find_all(\"p\")]\n",
    "            return \" \".join(paragraphs)  # Return all text as a single string\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_csv(self, csv_file):\n",
    "        \"\"\"Process CSV file and analyze extracted text.\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            link = row.get(\"Link\", \"\").strip()\n",
    "            text = row.get(\"Text\", \"\").strip()\n",
    "\n",
    "            if link:\n",
    "                print(f\"Scraping: {link}\")\n",
    "                web_text = self.scrape_webpage(link)\n",
    "                self.extract_entities_and_relationships(web_text, source=link)\n",
    "            \n",
    "            if text:\n",
    "                self.extract_entities_and_relationships(text, source=\"CSV Data\")\n",
    "\n",
    "    def visualize_relationships(self, output_file=\"country_network.html\"):\n",
    "        \"\"\"Visualize country relationships using Pyvis with sentiment-based colors.\"\"\"\n",
    "        net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", notebook=True, cdn_resources='in_line')\n",
    "\n",
    "        # Add nodes (countries)\n",
    "        countries = set()\n",
    "        for relation in self.relationships:\n",
    "            countries.add(relation['source'])\n",
    "            countries.add(relation['target'])\n",
    "\n",
    "        for country in countries:\n",
    "            net.add_node(country, label=country, color=\"blue\", size=15)\n",
    "\n",
    "        # Add edges with sentiment-based color\n",
    "        for relation in self.relationships:\n",
    "            sentiment = relation['sentiment']\n",
    "            if sentiment == \"POSITIVE\":\n",
    "                edge_color = \"green\"\n",
    "            elif sentiment == \"NEGATIVE\":\n",
    "                edge_color = \"red\"\n",
    "            else:\n",
    "                edge_color = \"gray\"\n",
    "\n",
    "            net.add_edge(relation['source'], relation['target'], width=2, color=edge_color, title=relation['sentence'])\n",
    "\n",
    "        # Save and show visualization\n",
    "        net.show(output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"/Users/benitaleonardi/Documents/news_excerpts_parsed_mini.csv\"\n",
    "\n",
    "    analyzer = CountrySentimentAnalyzer()\n",
    "\n",
    "    # Process CSV data (scrape web content and analyze text)\n",
    "    analyzer.process_csv(csv_file)\n",
    "\n",
    "    # Visualize the combined network from CSV data\n",
    "    analyzer.visualize_relationships(\"country_network_2.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e163833-bd37-4ffa-8e4d-ef34354788c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Hugging Face Transformers Sentiment Model\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "class PDFTextAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the text analyzer with NLP and data structures.\"\"\"\n",
    "        self.nlp = nlp  # Use loaded SpaCy model\n",
    "        self.entities = defaultdict(set)\n",
    "        self.relationships = []\n",
    "\n",
    "    def standardize_country_name(self, country):\n",
    "        \"\"\"Attempt to standardize country names using pycountry.\"\"\"\n",
    "        try:\n",
    "            return pycountry.countries.lookup(country).name\n",
    "        except LookupError:\n",
    "            return None  # Ignore non-country entities\n",
    "\n",
    "    def get_sentiment(self, sentence):\n",
    "        \"\"\"Analyze sentiment of a sentence using Transformers.\"\"\"\n",
    "        result = sentiment_pipeline(sentence)\n",
    "        sentiment_label = result[0]['label']  # 'POSITIVE' or 'NEGATIVE'\n",
    "        confidence = result[0]['score']\n",
    "\n",
    "        # If confidence is low, classify as NEUTRAL\n",
    "        if confidence < 0.7:\n",
    "            return 'NEUTRAL'\n",
    "        return sentiment_label\n",
    "\n",
    "    def extract_entities_and_relationships(self, text, source=None):\n",
    "        \"\"\"Extract country entities (GPE) and relationships from text.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        country_entities = set()\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"GPE\":\n",
    "                country_name = self.standardize_country_name(ent.text)\n",
    "                if country_name:  # Only add valid countries\n",
    "                    country_entities.add((country_name, source))\n",
    "\n",
    "        # Store valid countries\n",
    "        for country in country_entities:\n",
    "            self.entities['GPE'].add(country)\n",
    "\n",
    "        # Create relationships between countries in the same sentence\n",
    "        for sent in doc.sents:\n",
    "            sent_doc = self.nlp(sent.text)\n",
    "            entities_in_sent = [self.standardize_country_name(e.text) for e in sent_doc.ents if e.label_ == \"GPE\"]\n",
    "            entities_in_sent = [e for e in entities_in_sent if e]  # Remove None values\n",
    "\n",
    "            if len(entities_in_sent) >= 2:\n",
    "                sentiment = self.get_sentiment(sent.text)  # Get sentiment of the sentence\n",
    "                for i in range(len(entities_in_sent) - 1):\n",
    "                    self.relationships.append({\n",
    "                        'source': entities_in_sent[i],\n",
    "                        'target': entities_in_sent[i + 1],\n",
    "                        'sentence': sent.text,\n",
    "                        'sentiment': sentiment\n",
    "                    })\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_file):\n",
    "        \"\"\"Extract text from a PDF file using PyPDF2.\"\"\"\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:\n",
    "                text += extracted_text\n",
    "        return text\n",
    "\n",
    "    def process_directory(self, directory):\n",
    "        \"\"\"Process all PDFs in a directory.\"\"\"\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(directory, filename)\n",
    "                with open(pdf_path, \"rb\") as pdf_file:\n",
    "                    text = self.extract_text_from_pdf(pdf_file)\n",
    "                    self.extract_entities_and_relationships(text, source=filename)\n",
    "\n",
    "    def visualize_relationships(self, output_file=\"country_network.html\"):\n",
    "        \"\"\"Visualize country relationships using Pyvis with sentiment-based colors.\"\"\"\n",
    "        net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", notebook=True, cdn_resources='in_line')\n",
    "\n",
    "        # Add nodes (countries)\n",
    "        countries = set()\n",
    "        for relation in self.relationships:\n",
    "            countries.add(relation['source'])\n",
    "            countries.add(relation['target'])\n",
    "\n",
    "        for country in countries:\n",
    "            net.add_node(country, label=country, color=\"blue\", size=15)\n",
    "\n",
    "        # Add edges with sentiment-based color\n",
    "        for relation in self.relationships:\n",
    "            sentiment = relation['sentiment']\n",
    "            if sentiment == \"POSITIVE\":\n",
    "                edge_color = \"green\"\n",
    "            elif sentiment == \"NEGATIVE\":\n",
    "                edge_color = \"red\"\n",
    "            else:\n",
    "                edge_color = \"gray\"\n",
    "\n",
    "            net.add_edge(relation['source'], relation['target'], width=2, color=edge_color, title=relation['sentence'])\n",
    "\n",
    "        # Save and show visualization\n",
    "        net.show(output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_directory = \"/Users/benitaleonardi/Downloads/Datathon pdfs\"\n",
    "    analyzer = PDFTextAnalyzer()\n",
    "    analyzer.process_directory(pdf_directory)  # Process all PDFs in the specified directory\n",
    "    analyzer.visualize_relationships(\"country_network.html\")  # Visualize the relationships between countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6588a-61ee-4fc6-ac19-a802e07ba0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
