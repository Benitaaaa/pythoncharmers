{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/homebrew/Caskroom/miniconda/base/envs/myenv\n",
      "\n",
      "  added / updated specs:\n",
      "    - textblob\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    textblob-0.15.3            |             py_0         595 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         595 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  textblob           conda-forge/noarch::textblob-0.15.3-py_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2024.12.~ --> conda-forge/noarch::certifi-2024.12.14-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=a5fc6a1ed6def3096e3f453150203a5e64c275d7add90ae71265bcde5adc6021\n",
      "  Stored in directory: /Users/tiarahimawan/Library/Caches/pip/wheels/f9/72/27/74994e77200dae3d6aea2b546264500cee21f738c51241320b\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/homebrew/Caskroom/miniconda/base/envs/myenv\n",
      "\n",
      "  added / updated specs:\n",
      "    - pycountry\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    pycountry-24.6.1           |     pyhd8ed1ab_0         3.0 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pycountry          conda-forge/noarch::pycountry-24.6.1-pyhd8ed1ab_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2024.12.1~ --> pkgs/main/osx-arm64::certifi-2024.12.14-py312hca03da5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Using cached transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "Downloading torch-2.6.0-cp312-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m164.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:02\u001b[0m00:11\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m166.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m194.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.17.0 fsspec-2024.12.0 huggingface-hub-0.28.1 mpmath-1.3.0 safetensors-0.5.2 sympy-1.13.1 tokenizers-0.21.0 torch-2.6.0 transformers-4.48.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --default-timeout=100 transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import pycountry\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client at the top of your script\n",
    "client = OpenAI(\n",
    "    api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3273905641.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[57], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -m spacy download en_core_web_md\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_md (NOT SURE HOW TO DO THIS IN JUPYTERNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"pdf_dir\": \"pdfs\",\n",
    "        \"text_dir\": \"./processed/text/\",\n",
    "        \"summary_dir\": \"./processed/summaries/\",\n",
    "        \"relations_file\": \"./processed/relationships.csv\",\n",
    "        \"network_file\": \"./processed/network.html\"\n",
    "    },\n",
    "    \"gpt_model\": \"gpt-4o-mini\",\n",
    "    \"force_text_update\": False,\n",
    "    \"force_summary_update\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 1. Directory Setup\n",
    "# ------------------\n",
    "def setup_directories():\n",
    "    os.makedirs(CONFIG['paths']['text_dir'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['paths']['summary_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 2. Text Extraction (Run once)\n",
    "# ------------------\n",
    "def extract_all_texts():\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    pdf_files = [f for f in os.listdir(CONFIG['paths']['pdf_dir']) \n",
    "                if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files):\n",
    "        text_path = os.path.join(CONFIG['paths']['text_dir'], f\"{pdf_file}.txt\")\n",
    "        \n",
    "        if not CONFIG['force_text_update'] and os.path.exists(text_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            text = extract_pdf_text(os.path.join(CONFIG['paths']['pdf_dir'], pdf_file))\n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {pdf_file}: {str(e)}\")\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 3. GPT Summarization (Run once)\n",
    "# ------------------\n",
    "def generate_all_summaries():\n",
    "    print(\"Generating summaries...\")\n",
    "    text_files = [f for f in os.listdir(CONFIG['paths']['text_dir']) \n",
    "                 if f.endswith('.txt')]\n",
    "    \n",
    "    for text_file in tqdm(text_files):\n",
    "        summary_path = os.path.join(CONFIG['paths']['summary_dir'], \n",
    "                                  f\"{text_file}.json\")\n",
    "        \n",
    "        if not CONFIG['force_summary_update'] and os.path.exists(summary_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(os.path.join(CONFIG['paths']['text_dir'], text_file), 'r') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            summary = get_gpt_summary(text)\n",
    "            with open(summary_path, 'w') as f:\n",
    "                json.dump(summary, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to summarize {text_file}: {str(e)}\")\n",
    "\n",
    "def get_gpt_summary(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=CONFIG[\"gpt_model\"],\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Identify geopolitical entities (GPE) and organizations (ORG) in the text. Also, extract relationships between them:\\n{text[:6000]}\"\n",
    "            }],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return {\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"usage\": response.usage.dict()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"GPT Error: {str(e)}\")\n",
    "        return {\"content\": \"\", \"usage\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 4. Relationship Extraction (Can modify independently)\n",
    "# ------------------\n",
    "def extract_all_relations():\n",
    "    print(\"Extracting relationships...\")\n",
    "    all_relations = []\n",
    "    \n",
    "    text_files = [f for f in os.listdir(CONFIG['paths']['text_dir']) \n",
    "                 if f.endswith('.txt')]\n",
    "    \n",
    "    for text_file in tqdm(text_files):\n",
    "        try:\n",
    "            with open(os.path.join(CONFIG['paths']['text_dir'], text_file), 'r') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            relations = process_text_relations(text, os.path.basename(text_file))\n",
    "            all_relations.extend(relations)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {text_file}: {str(e)}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_relations)\n",
    "    df.to_csv(CONFIG['paths']['relations_file'], index=False)\n",
    "    return df\n",
    "\n",
    "# Country alias mapping to unify names\n",
    "country_aliases = {\n",
    "    \"usa\": \"United States\",\n",
    "    \"u.s.\": \"United States\",\n",
    "    \"us\": \"United States\",\n",
    "    \"united states\": \"United States\",\n",
    "    \"the united states\": \"United States\",\n",
    "    \"america\": \"United States\",\n",
    "    \"great britain\": \"United Kingdom\",\n",
    "    \"u.k.\": \"United Kingdom\",\n",
    "    \"uk\": \"United Kingdom\",\n",
    "    \"russia\": \"Russian Federation\",\n",
    "    \"iran\": \"Islamic Republic of Iran\",\n",
    "    \"north korea\": \"Democratic People's Republic of Korea\",\n",
    "    \"south korea\": \"Republic of Korea\",\n",
    "    \"china prc\": \"China\",\n",
    "    \"people's republic of china\": \"China\"\n",
    "}\n",
    "\n",
    "def normalize_country(name):\n",
    "    \"\"\"Normalize country names by handling aliases and capitalization.\"\"\"\n",
    "    name_lower = name.lower().strip()  # Convert to lowercase for consistent matching\n",
    "    \n",
    "    if name_lower in country_aliases:\n",
    "        return country_aliases[name_lower]  # Return mapped name if exists\n",
    "    \n",
    "    # Try finding the country in pycountry (case insensitive)\n",
    "    try:\n",
    "        country = pycountry.countries.lookup(name)\n",
    "        return country.name  # Return official name\n",
    "    except LookupError:\n",
    "        return name  # Return original if not found\n",
    "\n",
    "# def process_text_relations(text, source_file):\n",
    "#     doc = nlp(text)\n",
    "#     relations = []\n",
    "\n",
    "#     for sent in doc.sents:\n",
    "#         entities = {ent.text: ent.label_ for ent in sent.ents}\n",
    "\n",
    "#         for token in sent:\n",
    "#             if token.pos_ == \"VERB\":\n",
    "#                 subj_entities = [child.text for child in token.children \n",
    "#                                 if child.dep_ in (\"nsubj\", \"nsubjpass\") \n",
    "#                                 and entities.get(child.text) in [\"GPE\", \"ORG\"]]\n",
    "\n",
    "#                 obj_entities = [child.text for child in token.children \n",
    "#                                 if child.dep_ in (\"dobj\", \"pobj\") \n",
    "#                                 and entities.get(child.text) in [\"GPE\", \"ORG\"]]\n",
    "\n",
    "#                 if subj_entities and obj_entities:\n",
    "#                     relations.append({\n",
    "#                         'source': subj_entities[0],\n",
    "#                         'target': obj_entities[0],\n",
    "#                         'verb': token.lemma_,\n",
    "#                         'sentence': sent.text,\n",
    "#                         'sentiment': analyze_sentiment(sent.text),\n",
    "#                         'source_file': source_file\n",
    "#                     })\n",
    "\n",
    "#     return relations\n",
    "def process_text_relations(text, source_file):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        gpe_entities = [normalize_country(ent.text) for ent in sent.ents if ent.label_ == \"GPE\"]\n",
    "        verbs = [token.lemma_ for token in sent if token.pos_ == \"VERB\"]\n",
    "        \n",
    "        if len(gpe_entities) < 2:\n",
    "            continue  # Skip if no country relationships\n",
    "        \n",
    "        relationship_type = verbs[0] if verbs else \"unknown\"\n",
    "        \n",
    "        for i in range(len(gpe_entities)):\n",
    "            for j in range(i + 1, len(gpe_entities)):  # Avoid self-links\n",
    "                relations.append({\n",
    "                    'source': gpe_entities[i],\n",
    "                    'target': gpe_entities[j],\n",
    "                    'verb': relationship_type,\n",
    "                    'sentence': sent.text,\n",
    "                    'sentiment': analyze_sentiment(sent.text),\n",
    "                    'source_file': source_file\n",
    "                })\n",
    "    \n",
    "    return relations\n",
    "\n",
    "def extract_entity(token):\n",
    "    \"\"\"Get full entity span for a token\"\"\"\n",
    "    if token.ent_iob_ != \"O\":  # Check if token is part of an entity\n",
    "        ent = [e for e in token.doc.ents if token.i >= e.start and token.i < e.end]\n",
    "        if ent and ent[0].label_ in [\"GPE\", \"ORG\"]:\n",
    "            return ent[0].text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd2ff12cf184317944a89b3b1d4913a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9939b1afbf494eda8ed5654371e71655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/cardiffnlp/twitter-roberta-base-sentiment/c37a3484c55954cd75b336a85f1e0c023ae874f3a73b05d2418dd04828e293b1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1738255962&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODI1NTk2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9jYXJkaWZmbmxwL3R3aXR0ZXItcm9iZXJ0YS1iYXNlLXNlbnRpbWVudC9jMzdhMzQ4NGM1NTk1NGNkNzViMzM2YTg1ZjFlMGMwMjNhZTg3NGYzYTczYjA1ZDI0MThkZDA0ODI4ZTI5M2IxP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=N5aAN6PnrHxQz92nju6cFsm6S5d9uvFhRVjMZHDhequ1m9LoaWyidyZ5Z9xw6o4yWbFM2VjtXJ4WwTfdpLaALhVe-GRZyCMBVPxZkccv7kvPM9mS5rcQpXxz7a89EYeP%7EZ4bCeK6%7EsDZyH8kkCrSCY2rw%7E3xDS-8J61O4-0N%7EJCH803HganGc%7EbZsKsjarOiCXACaWRFXKVhZ3oCl4pw2omf4w--OrwA3HycweysiYSGwuXEI%7EBv2bXvj1t-N6IxKPcavVUmMSH9B0C8kr9UCiEtaOpAiqKWRbKOywsbExnK8pMs3duDL4%7EP8YwBnJSWl6vvHWXwZGxAXGYOmQK7cw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df704e97d62a4e5b8448150d3e418475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  11%|#         | 52.4M/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/cardiffnlp/twitter-roberta-base-sentiment/c37a3484c55954cd75b336a85f1e0c023ae874f3a73b05d2418dd04828e293b1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1738255962&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODI1NTk2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9jYXJkaWZmbmxwL3R3aXR0ZXItcm9iZXJ0YS1iYXNlLXNlbnRpbWVudC9jMzdhMzQ4NGM1NTk1NGNkNzViMzM2YTg1ZjFlMGMwMjNhZTg3NGYzYTczYjA1ZDI0MThkZDA0ODI4ZTI5M2IxP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=N5aAN6PnrHxQz92nju6cFsm6S5d9uvFhRVjMZHDhequ1m9LoaWyidyZ5Z9xw6o4yWbFM2VjtXJ4WwTfdpLaALhVe-GRZyCMBVPxZkccv7kvPM9mS5rcQpXxz7a89EYeP%7EZ4bCeK6%7EsDZyH8kkCrSCY2rw%7E3xDS-8J61O4-0N%7EJCH803HganGc%7EbZsKsjarOiCXACaWRFXKVhZ3oCl4pw2omf4w--OrwA3HycweysiYSGwuXEI%7EBv2bXvj1t-N6IxKPcavVUmMSH9B0C8kr9UCiEtaOpAiqKWRbKOywsbExnK8pMs3duDL4%7EP8YwBnJSWl6vvHWXwZGxAXGYOmQK7cw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6915ea0934b843bf88da8a8f5b75161b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  27%|##7       | 136M/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc6ffe83b35412199514687b5fcca77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbbbc21dfc94c6899a29b95b302eb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbf0a1e56124f6caab34bb20aa27773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe23c1104224e3ba94b43d1d46fa27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# # ‚úÖ Load RoBERTa sentiment model\n",
    "# roberta_sentiment = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_sentiment(sentence):\n",
    "#     \"\"\"RoBERTa-based sentiment analysis only\"\"\"\n",
    "#     try:\n",
    "#         roberta_result = roberta_sentiment(sentence)[0]\n",
    "#         transformer_label = roberta_result['label'].lower()\n",
    "\n",
    "#         if transformer_label == \"positive\":\n",
    "#             return \"positive\"\n",
    "#         elif transformer_label == \"negative\":\n",
    "#             return \"negative\"\n",
    "#         else:\n",
    "#             return \"neutral\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è RoBERTa Sentiment Analysis Error: {str(e)}\")\n",
    "#         return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Define correct label mapping (RoBERTa's output labels)\n",
    "roberta_label_map = {\n",
    "    \"LABEL_0\": \"negative\",  # RoBERTa classifies negative as LABEL_0\n",
    "    \"LABEL_1\": \"neutral\",   # RoBERTa classifies neutral as LABEL_1\n",
    "    \"LABEL_2\": \"positive\"   # RoBERTa classifies positive as LABEL_2\n",
    "}\n",
    "\n",
    "def analyze_sentiment(sentence):\n",
    "    \"\"\"Improved RoBERTa-based sentiment analysis with correct label mapping\"\"\"\n",
    "    try:\n",
    "        # ‚úÖ Ensure input is not too long\n",
    "        max_length = 512  # Transformer models have a token limit\n",
    "        sentence = sentence[:max_length]\n",
    "\n",
    "        # ‚úÖ Get sentiment prediction\n",
    "        roberta_result = roberta_sentiment(sentence)[0]\n",
    "        transformer_label = roberta_label_map[roberta_result['label']]  # Convert RoBERTa labels\n",
    "\n",
    "        # ‚úÖ Apply confidence threshold\n",
    "        confidence_score = roberta_result['score']  # Probability score from RoBERTa\n",
    "        if confidence_score < 0.6:  # If confidence is low, default to neutral\n",
    "            return \"neutral\"\n",
    "\n",
    "        return transformer_label\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è RoBERTa Sentiment Analysis Error: {str(e)}\")\n",
    "        return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_relationships():\n",
    "    df = pd.read_csv(CONFIG['paths']['relations_file'])\n",
    "\n",
    "    # Print top 20 most connected countries\n",
    "    print(\"\\nüîç Top 20 Most Frequent Countries in Relations:\")\n",
    "    print(df['source'].value_counts().head(20))\n",
    "\n",
    "    print(\"\\nüîç Sample Relationships from CSV:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of real country names\n",
    "real_countries = {country.name for country in pycountry.countries}\n",
    "\n",
    "def is_valid_country(name):\n",
    "    return name in real_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_majority_sentiment(sentiments):\n",
    "    \"\"\"Determines the majority sentiment from a list of sentiment values.\"\"\"\n",
    "    count = Counter(sentiments)\n",
    "    \n",
    "    # If one sentiment clearly dominates\n",
    "    if count[\"positive\"] > count[\"negative\"]:\n",
    "        return \"positive\"\n",
    "    elif count[\"negative\"] > count[\"positive\"]:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "\n",
    "def get_edge_color(sentiment):\n",
    "    if sentiment == \"positive\":\n",
    "        return \"#3DED97\"  # Green for positive relations\n",
    "    elif sentiment == \"negative\":\n",
    "        return \"#ff4c4c\"  # Red for negative relations\n",
    "    else:\n",
    "        return \"#cccccc\"  # Gray for neutral relations\n",
    "\n",
    "def generate_gpe_relationship_graph():\n",
    "    df = pd.read_csv(CONFIG['paths']['relations_file'])\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        country1 = normalize_country(row['source'])\n",
    "        country2 = normalize_country(row['target'])\n",
    "\n",
    "        # ‚úÖ Skip if source and target are the same (self-loops)\n",
    "        if country1 == country2:\n",
    "            continue\n",
    "\n",
    "        if not is_valid_country(country1) or not is_valid_country(country2):\n",
    "            continue\n",
    "        \n",
    "        verb, sentiment = row['verb'], row['sentiment']\n",
    "        \n",
    "        if G.has_edge(country1, country2):\n",
    "            G[country1][country2]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(country1, country2, verb=verb, sentiment=sentiment, weight=1)\n",
    "\n",
    "    net = Network(notebook=True, width=\"1200px\", height=\"800px\", bgcolor='#222222', font_color='white', cdn_resources='in_line')\n",
    "\n",
    "    for node, degree in dict(G.degree()).items():\n",
    "        net.add_node(node, size=max(10, min(degree * 4, 60)), color=\"#FFD700\", label=node)\n",
    "\n",
    "    for src, dst, data in G.edges(data=True):\n",
    "        edge_width = max(1, min(data[\"weight\"] * 0.5, 10))  # Normalize thickness\n",
    "        net.add_edge(src, dst, title=data['verb'], width=edge_width, color=get_edge_color(data['sentiment']))\n",
    "\n",
    "    net.barnes_hut(gravity=-500, central_gravity=0.3, spring_length=100, damping=0.8)\n",
    "\n",
    "    output_file = CONFIG['paths']['network_file']\n",
    "    net.show(output_file)\n",
    "    print(f\"‚úÖ Relationship network saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these only once\n",
    "# extract_all_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [00:00<00:00, 33031.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run these only once\n",
    "generate_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting relationships...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [03:27<00:00,  4.72s/it]\n"
     ]
    }
   ],
   "source": [
    "relations_df = extract_all_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 20 Most Frequent Countries in Relations:\n",
      "source\n",
      "United States               153\n",
      "Kosovo                       30\n",
      "Japan                        26\n",
      "France                       25\n",
      "Afghanistan                  24\n",
      "Islamic Republic of Iran     22\n",
      "Pakistan                     19\n",
      "Yemen                        17\n",
      "Washington                   15\n",
      "Somalia                      15\n",
      "Sahel                        13\n",
      "Russian Federation           11\n",
      "China                        11\n",
      "Australia                    11\n",
      "Italy                         9\n",
      "METI                          8\n",
      "India                         8\n",
      "Turkey                        7\n",
      "SECRET//COMINT//REL           7\n",
      "Iraq                          6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Sample Relationships from CSV:\n",
      "               source     target    verb  \\\n",
      "0              Kosovo     Kosovo  regard   \n",
      "1       United States      Japan     bug   \n",
      "2       United States      Japan     bug   \n",
      "3               Japan      Japan     bug   \n",
      "4  the United States'  Australia    mark   \n",
      "\n",
      "                                            sentence sentiment  source_file  \n",
      "0   \\n1\\nUNITED NATIONS \\n   United Nations Inter...   neutral   45.pdf.txt  \n",
      "1  NSA Global SIGINT Highlights \\nUS Bugged Japan...   neutral  107.pdf.txt  \n",
      "2  NSA Global SIGINT Highlights \\nUS Bugged Japan...   neutral  107.pdf.txt  \n",
      "3  NSA Global SIGINT Highlights \\nUS Bugged Japan...   neutral  107.pdf.txt  \n",
      "4  The report is marked for sharing \\nwith the Un...   neutral  107.pdf.txt  \n"
     ]
    }
   ],
   "source": [
    "debug_relationships()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./processed/network.html\n",
      "‚úÖ Relationship network saved to ./processed/network.html\n"
     ]
    }
   ],
   "source": [
    "generate_gpe_relationship_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
