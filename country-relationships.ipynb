{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/homebrew/Caskroom/miniconda/base/envs/myenv\n",
      "\n",
      "  added / updated specs:\n",
      "    - pycountry\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    pycountry-24.6.1           |     pyhd8ed1ab_0         3.0 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pycountry          conda-forge/noarch::pycountry-24.6.1-pyhd8ed1ab_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2024.12.1~ --> pkgs/main/osx-arm64::certifi-2024.12.14-py312hca03da5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import pycountry\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client at the top of your script\n",
    "client = OpenAI(\n",
    "    api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3273905641.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[57], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -m spacy download en_core_web_md\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_md (NOT SURE HOW TO DO THIS IN JUPYTERNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"pdf_dir\": \"pdfs\",\n",
    "        \"text_dir\": \"./processed/text/\",\n",
    "        \"summary_dir\": \"./processed/summaries/\",\n",
    "        \"relations_file\": \"./processed/relationships.csv\",\n",
    "        \"network_file\": \"./processed/network.html\"\n",
    "    },\n",
    "    \"gpt_model\": \"gpt-4o-mini\",\n",
    "    \"force_text_update\": False,\n",
    "    \"force_summary_update\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 1. Directory Setup\n",
    "# ------------------\n",
    "def setup_directories():\n",
    "    os.makedirs(CONFIG['paths']['text_dir'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['paths']['summary_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 2. Text Extraction (Run once)\n",
    "# ------------------\n",
    "def extract_all_texts():\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    pdf_files = [f for f in os.listdir(CONFIG['paths']['pdf_dir']) \n",
    "                if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files):\n",
    "        text_path = os.path.join(CONFIG['paths']['text_dir'], f\"{pdf_file}.txt\")\n",
    "        \n",
    "        if not CONFIG['force_text_update'] and os.path.exists(text_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            text = extract_pdf_text(os.path.join(CONFIG['paths']['pdf_dir'], pdf_file))\n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {pdf_file}: {str(e)}\")\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 3. GPT Summarization (Run once)\n",
    "# ------------------\n",
    "def generate_all_summaries():\n",
    "    print(\"Generating summaries...\")\n",
    "    text_files = [f for f in os.listdir(CONFIG['paths']['text_dir']) \n",
    "                 if f.endswith('.txt')]\n",
    "    \n",
    "    for text_file in tqdm(text_files):\n",
    "        summary_path = os.path.join(CONFIG['paths']['summary_dir'], \n",
    "                                  f\"{text_file}.json\")\n",
    "        \n",
    "        if not CONFIG['force_summary_update'] and os.path.exists(summary_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(os.path.join(CONFIG['paths']['text_dir'], text_file), 'r') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            summary = get_gpt_summary(text)\n",
    "            with open(summary_path, 'w') as f:\n",
    "                json.dump(summary, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to summarize {text_file}: {str(e)}\")\n",
    "\n",
    "def get_gpt_summary(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=CONFIG[\"gpt_model\"],\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Identify geopolitical entities and relationships:\\n{text[:6000]}\"\n",
    "            }],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return {\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"usage\": response.usage.dict()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"GPT Error: {str(e)}\")\n",
    "        return {\"content\": \"\", \"usage\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 4. Relationship Extraction (Can modify independently)\n",
    "# ------------------\n",
    "def extract_all_relations():\n",
    "    print(\"Extracting relationships...\")\n",
    "    all_relations = []\n",
    "    \n",
    "    text_files = [f for f in os.listdir(CONFIG['paths']['text_dir']) \n",
    "                 if f.endswith('.txt')]\n",
    "    \n",
    "    for text_file in tqdm(text_files):\n",
    "        try:\n",
    "            with open(os.path.join(CONFIG['paths']['text_dir'], text_file), 'r') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            relations = process_text_relations(text, os.path.basename(text_file))\n",
    "            all_relations.extend(relations)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {text_file}: {str(e)}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_relations)\n",
    "    df.to_csv(CONFIG['paths']['relations_file'], index=False)\n",
    "    return df\n",
    "\n",
    "# Country alias mapping to unify names\n",
    "country_aliases = {\n",
    "    \"usa\": \"United States\",\n",
    "    \"u.s.\": \"United States\",\n",
    "    \"us\": \"United States\",\n",
    "    \"united states\": \"United States\",\n",
    "    \"the united states\": \"United States\",\n",
    "    \"america\": \"United States\",\n",
    "    \"great britain\": \"United Kingdom\",\n",
    "    \"u.k.\": \"United Kingdom\",\n",
    "    \"uk\": \"United Kingdom\",\n",
    "    \"russia\": \"Russian Federation\",\n",
    "    \"iran\": \"Islamic Republic of Iran\",\n",
    "    \"north korea\": \"Democratic People's Republic of Korea\",\n",
    "    \"south korea\": \"Republic of Korea\",\n",
    "    \"china prc\": \"China\",\n",
    "    \"people's republic of china\": \"China\"\n",
    "}\n",
    "\n",
    "def normalize_country(name):\n",
    "    \"\"\"Normalize country names by handling aliases and capitalization.\"\"\"\n",
    "    name_lower = name.lower().strip()  # Convert to lowercase for consistent matching\n",
    "    \n",
    "    if name_lower in country_aliases:\n",
    "        return country_aliases[name_lower]  # Return mapped name if exists\n",
    "    \n",
    "    # Try finding the country in pycountry (case insensitive)\n",
    "    try:\n",
    "        country = pycountry.countries.lookup(name)\n",
    "        return country.name  # Return official name\n",
    "    except LookupError:\n",
    "        return name  # Return original if not found\n",
    "\n",
    "def process_text_relations(text, source_file):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        gpe_entities = [normalize_country(ent.text) for ent in sent.ents if ent.label_ == \"GPE\"]\n",
    "        verbs = [token.lemma_ for token in sent if token.pos_ == \"VERB\"]\n",
    "        \n",
    "        if len(gpe_entities) < 2:\n",
    "            continue  # Skip if no country relationships\n",
    "        \n",
    "        relationship_type = verbs[0] if verbs else \"unknown\"\n",
    "        \n",
    "        for i in range(len(gpe_entities)):\n",
    "            for j in range(i + 1, len(gpe_entities)):  # Avoid self-links\n",
    "                relations.append({\n",
    "                    'source': gpe_entities[i],\n",
    "                    'target': gpe_entities[j],\n",
    "                    'verb': relationship_type,\n",
    "                    'sentence': sent.text,\n",
    "                    'sentiment': analyze_sentiment(sent.text),\n",
    "                    'source_file': source_file\n",
    "                })\n",
    "    \n",
    "    return relations\n",
    "\n",
    "def extract_entity(token):\n",
    "    \"\"\"Get full entity span for a token\"\"\"\n",
    "    if token.ent_iob_ != \"O\":  # Check if token is part of an entity\n",
    "        ent = [e for e in token.doc.ents if token.i >= e.start and token.i < e.end]\n",
    "        if ent and ent[0].label_ in [\"GPE\", \"ORG\"]:\n",
    "            return ent[0].text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# 5. Sentiment Analysis (Modify independently)\n",
    "# ------------------\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# âœ… Expanded conflict & cooperation word lists\n",
    "negative_words = {\"accuse\", \"violate\", \"sanction\", \"threaten\", \"criticize\", \"blame\", \"condemn\", \"warn\", \"punish\", \"boycott\"}\n",
    "positive_words = {\"support\", \"aid\", \"collaborate\", \"cooperate\", \"assist\", \"ally\", \"negotiate\", \"agree\", \"sign\", \"trade\"}\n",
    "\n",
    "# âœ… Expanded negation handling\n",
    "negation_words = {\"no\", \"not\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"hardly\", \"scarcely\", \"barely\", \"without\"}\n",
    "\n",
    "def analyze_sentiment(sentence):\n",
    "    \"\"\"Enhanced sentiment analysis with negation, rule-based logic, and fallback LLM\"\"\"\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # 1ï¸âƒ£ Check for negation modifying sentiment words\n",
    "    for token in doc:\n",
    "        if token.lemma_ in negative_words and any(child.text.lower() in negation_words for child in token.children):\n",
    "            return \"positive\"  # Negation flips meaning\n",
    "        if token.lemma_ in positive_words and any(child.text.lower() in negation_words for child in token.children):\n",
    "            return \"negative\"  # Negation flips meaning\n",
    "\n",
    "    # 2ï¸âƒ£ Direct sentiment classification using keyword-based approach\n",
    "    for token in doc:\n",
    "        if token.lemma_ in negative_words and token.dep_ == \"ROOT\":\n",
    "            return \"negative\"\n",
    "        if token.lemma_ in positive_words and token.dep_ == \"ROOT\":\n",
    "            return \"positive\"\n",
    "\n",
    "    # 3ï¸âƒ£ Fallback to VADER Sentiment Analysis (adjusted for neutrality)\n",
    "    vader_score = sentiment_analyzer.polarity_scores(sentence)['compound']\n",
    "    if vader_score >= 0.3:  # Raised threshold for positive classification\n",
    "        return \"positive\"\n",
    "    elif vader_score <= -0.2:  # Lowered threshold for negative classification\n",
    "        return \"negative\"\n",
    "\n",
    "    # 4ï¸âƒ£ Explicitly return \"neutral\" if no strong sentiment is detected\n",
    "    return \"neutral\"\n",
    "\n",
    "# âœ… Optimize LLM Sentiment Calls (only when necessary)\n",
    "def query_llm_sentiment(sentence):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Classify this sentence's sentiment as positive, negative, or neutral:\n",
    "                Sentence: \"{sentence}\"\n",
    "                Respond ONLY with one word:\"\"\"\n",
    "            }],\n",
    "            max_tokens=5,  # Lower token limit for efficiency\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return response.choices[0].message.content.strip().lower()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Sentiment LLM Error: {str(e)}\")\n",
    "        return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_relationships():\n",
    "    df = pd.read_csv(CONFIG['paths']['relations_file'])\n",
    "\n",
    "    # Print top 20 most connected countries\n",
    "    print(\"\\nðŸ” Top 20 Most Frequent Countries in Relations:\")\n",
    "    print(df['source'].value_counts().head(20))\n",
    "\n",
    "    print(\"\\nðŸ” Sample Relationships from CSV:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of real country names\n",
    "real_countries = {country.name for country in pycountry.countries}\n",
    "\n",
    "def is_valid_country(name):\n",
    "    return name in real_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "\n",
    "def get_edge_color(sentiment):\n",
    "    if sentiment == \"positive\":\n",
    "        return \"#3DED97\"  # Green for positive relations\n",
    "    elif sentiment == \"negative\":\n",
    "        return \"#ff4c4c\"  # Red for negative relations\n",
    "    else:\n",
    "        return \"#cccccc\"  # Gray for neutral relations\n",
    "\n",
    "def generate_gpe_relationship_graph():\n",
    "    df = pd.read_csv(CONFIG['paths']['relations_file'])\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        country1 = normalize_country(row['source'])\n",
    "        country2 = normalize_country(row['target'])\n",
    "\n",
    "        # âœ… Skip if source and target are the same (self-loops)\n",
    "        if country1 == country2:\n",
    "            continue\n",
    "\n",
    "        if not is_valid_country(country1) or not is_valid_country(country2):\n",
    "            continue\n",
    "        \n",
    "        verb, sentiment = row['verb'], row['sentiment']\n",
    "        \n",
    "        if G.has_edge(country1, country2):\n",
    "            G[country1][country2]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(country1, country2, verb=verb, sentiment=sentiment, weight=1)\n",
    "\n",
    "    net = Network(notebook=True, width=\"1200px\", height=\"800px\", bgcolor='#222222', font_color='white', cdn_resources='in_line')\n",
    "\n",
    "    for node, degree in dict(G.degree()).items():\n",
    "        net.add_node(node, size=max(10, min(degree * 4, 60)), color=\"#FFD700\", label=node)\n",
    "\n",
    "    for src, dst, data in G.edges(data=True):\n",
    "        edge_width = max(1, min(data[\"weight\"] * 0.5, 10))  # Normalize thickness\n",
    "        net.add_edge(src, dst, title=data['verb'], width=edge_width, color=get_edge_color(data['sentiment']))\n",
    "\n",
    "    net.barnes_hut(gravity=-500, central_gravity=0.3, spring_length=100, damping=0.8)\n",
    "\n",
    "    output_file = CONFIG['paths']['network_file']\n",
    "    net.show(output_file)\n",
    "    print(f\"âœ… Relationship network saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting relationships...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [03:17<00:00,  4.50s/it]\n"
     ]
    }
   ],
   "source": [
    "setup_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these only once\n",
    "# extract_all_texts()\n",
    "# generate_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df = extract_all_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top 20 Most Frequent Countries in Relations:\n",
      "source\n",
      "United States               153\n",
      "Kosovo                       30\n",
      "Japan                        26\n",
      "France                       25\n",
      "Afghanistan                  24\n",
      "Islamic Republic of Iran     22\n",
      "Pakistan                     19\n",
      "Yemen                        17\n",
      "Washington                   15\n",
      "Somalia                      15\n",
      "Sahel                        13\n",
      "Russian Federation           11\n",
      "China                        11\n",
      "Australia                    11\n",
      "Italy                         9\n",
      "METI                          8\n",
      "India                         8\n",
      "Turkey                        7\n",
      "SECRET//COMINT//REL           7\n",
      "Iraq                          6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ” Sample Relationships from CSV:\n",
      "               source     target    verb  \\\n",
      "0              Kosovo     Kosovo  regard   \n",
      "1       United States      Japan     bug   \n",
      "2       United States      Japan     bug   \n",
      "3               Japan      Japan     bug   \n",
      "4  the United States'  Australia    mark   \n",
      "\n",
      "                                            sentence sentiment  source_file  \n",
      "0   \\n1\\nUNITED NATIONS \\n   United Nations Inter...  positive   45.pdf.txt  \n",
      "1  NSA Global SIGINT Highlights \\nUS Bugged Japan...  positive  107.pdf.txt  \n",
      "2  NSA Global SIGINT Highlights \\nUS Bugged Japan...  positive  107.pdf.txt  \n",
      "3  NSA Global SIGINT Highlights \\nUS Bugged Japan...  positive  107.pdf.txt  \n",
      "4  The report is marked for sharing \\nwith the Un...  positive  107.pdf.txt  \n"
     ]
    }
   ],
   "source": [
    "debug_relationships()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./processed/network.html\n",
      "âœ… Relationship network saved to ./processed/network.html\n"
     ]
    }
   ],
   "source": [
    "generate_gpe_relationship_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
