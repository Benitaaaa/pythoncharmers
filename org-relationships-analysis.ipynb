{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd111c56",
   "metadata": {},
   "source": [
    "# Organization Relationships Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eea3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756f761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.2-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.2-cp39-abi3-win_amd64.whl (16.5 MB)\n",
      "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.8/16.5 MB 12.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.7/16.5 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.8/16.5 MB 12.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 11.0/16.5 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.5/16.5 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.5/16.5 MB 16.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d49c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\asyncio\\base_events.py\", line 640, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\asyncio\\base_events.py\", line 1992, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Davina\\AppData\\Local\\Temp\\ipykernel_8116\\3964151261.py\", line 2, in <module>\n",
      "    import spacy\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\thinc\\types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\thinc\\compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f5ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model for named entity recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load RoBERTa sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"pdf_dir\": \"./pdfs\",  # Directory containing PDFs\n",
    "        \"output_json\": \"./processed/organizations-relationships.csv\",  # Output JSON file\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e78c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdfs(pdf_dir):\n",
    "    texts = []\n",
    "    file_sources = []\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(pdf_dir, filename)\n",
    "            with fitz.open(filepath) as doc:\n",
    "                text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "                texts.append((text, filename))  # Store text with filename\n",
    "    return texts\n",
    "\n",
    "# Function to extract organization names and relationships\n",
    "def extract_relationships(text, filename):\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    \n",
    "    for sent in doc.sents:  # Process sentence by sentence\n",
    "        orgs = [ent.text for ent in sent.ents if ent.label_ == \"ORG\"]\n",
    "        verbs = [token.lemma_ for token in sent if token.pos_ == \"VERB\"]\n",
    "        \n",
    "        if len(orgs) >= 2 and verbs:  # Ensure at least two organizations and a verb\n",
    "            sentiment_result = sentiment_pipeline(sent.text[:512])[0][\"label\"]  # Process first 512 tokens\n",
    "            sentiment_label = \"neutral\"\n",
    "            if sentiment_result == \"negative\":\n",
    "                sentiment_label = \"negative\"\n",
    "            elif sentiment_result == \"positive\":\n",
    "                sentiment_label = \"positive\"\n",
    "            \n",
    "            relationships.append([\n",
    "                orgs[0],  # source organization\n",
    "                orgs[1],  # target organization\n",
    "                verbs[0],  # first verb found\n",
    "                sent.text.strip(),  # full sentence\n",
    "                sentiment_label,  # sentiment\n",
    "                filename  # source file\n",
    "            ])\n",
    "    return relationships\n",
    "\n",
    "# Function to save relationships as CSV\n",
    "def save_relationships_to_csv(relationships, output_path):\n",
    "    df = pd.DataFrame(relationships, columns=[\"source\", \"target\", \"verb\", \"sentence\", \"sentiment\", \"source_file\"])\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# Function to visualize organization relationships\n",
    "def visualize_relationships(relationships):\n",
    "    G = nx.Graph()\n",
    "    for org1, org2 in relationships:\n",
    "        G.add_edge(org1, org2)\n",
    "    net = Network(notebook=True, directed=False)\n",
    "    net.from_nx(G)\n",
    "    net.show(\"organization_network.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a642438",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m texts_with_sources \u001b[38;5;241m=\u001b[39m extract_text_from_pdfs(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, filename \u001b[38;5;129;01min\u001b[39;00m texts_with_sources:\n\u001b[1;32m----> 5\u001b[0m     all_relationships\u001b[38;5;241m.\u001b[39mextend(extract_relationships(text, filename))\n\u001b[0;32m      7\u001b[0m save_relationships_to_csv(all_relationships, CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mextract_relationships\u001b[1;34m(text, filename)\u001b[0m\n\u001b[0;32m     20\u001b[0m verbs \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sent \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERB\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(orgs) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbs:  \u001b[38;5;66;03m# Ensure at least two organizations and a verb\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     sentiment_result \u001b[38;5;241m=\u001b[39m sentiment_pipeline(sent\u001b[38;5;241m.\u001b[39mtext[:\u001b[38;5;241m512\u001b[39m])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Process first 512 tokens\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     sentiment_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentiment_result \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[1;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\transformers\\pipelines\\base.py:1362\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1355\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         )\n\u001b[0;32m   1360\u001b[0m     )\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\transformers\\pipelines\\base.py:1370\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1368\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1369\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m-> 1370\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Davina\\anaconda3\\envs\\smubia_datathon\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:213\u001b[0m, in \u001b[0;36mTextClassificationPipeline.postprocess\u001b[1;34m(self, model_outputs, function_to_apply, top_k, _legacy)\u001b[0m\n\u001b[0;32m    209\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# To enable using fp16 and bf16\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "all_relationships = []\n",
    "texts_with_sources = extract_text_from_pdfs(CONFIG[\"paths\"][\"pdf_dir\"])\n",
    "\n",
    "for text, filename in texts_with_sources:\n",
    "    all_relationships.extend(extract_relationships(text, filename))\n",
    "\n",
    "save_relationships_to_csv(all_relationships, CONFIG[\"paths\"][\"output_csv\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smubia_datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
