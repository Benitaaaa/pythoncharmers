{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd111c56",
   "metadata": {},
   "source": [
    "# Organization Relationships Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install openai spacy requests pandas networkx pyvis fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d49c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline\n",
    "from pyvis.network import Network\n",
    "import pycountry\n",
    "import pycountry_convert as pc  # For mapping countries to continents\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "312e3f9b-a2c6-4432-8fbb-8f61aef8599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 149\u001b[0m\n\u001b[1;32m    146\u001b[0m all_relationships \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, filename \u001b[38;5;129;01min\u001b[39;00m texts_with_sources:\n\u001b[0;32m--> 149\u001b[0m     relationships \u001b[38;5;241m=\u001b[39m extract_organization_relationships(text, filename)\n\u001b[1;32m    150\u001b[0m     all_relationships\u001b[38;5;241m.\u001b[39mextend(relationships)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Save relationships to CSV\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 92\u001b[0m, in \u001b[0;36mextract_organization_relationships\u001b[0;34m(text, filename)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m     91\u001b[0m     sent_doc \u001b[38;5;241m=\u001b[39m nlp(sent\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m---> 92\u001b[0m     entities_in_sent \u001b[38;5;241m=\u001b[39m [normalize_org_name(e\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m sent_doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORG\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     entities_in_sent \u001b[38;5;241m=\u001b[39m [e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m entities_in_sent \u001b[38;5;28;01mif\u001b[39;00m e]  \u001b[38;5;66;03m# Remove None values\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(entities_in_sent) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m, in \u001b[0;36mnormalize_org_name\u001b[0;34m(org_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: org_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m}\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     37\u001b[0m         data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    694\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    695\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    200\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    202\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    203\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load spaCy model for Named Entity Recognition (NER) ~ Detects organization names in text.\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Load Hugging Face sentiment analysis model ~ Analyzes the emotional tone of text.\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# OpenAI API Key (Set this securely in your environment)\n",
    "client = openai.OpenAI(api_key=\"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\") \n",
    "\n",
    "# Function to summarize text using OpenAI GPT-4o-mini\n",
    "def summarize_text(text):\n",
    "    prompt = \"\"\"Identify the main points in the article provided.\n",
    "    Given these main points, find relationships involving entities of type Organization.\n",
    "    \\n\\nArticle:\\n\"\"\" + text[:4000]  # Truncate to avoid API limits\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            store=True,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        summary = completion.choices[0].message.content\n",
    "        return summary.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return text  # Fallback to original text if API fails\n",
    "\n",
    "# Function to normalize organization names using DBpedia Spotlight\n",
    "def normalize_org_name(org_name):\n",
    "    url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    params = {\"text\": org_name, \"confidence\": 0.5}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"Resources\" in data:\n",
    "                return data[\"Resources\"][0][\"@URI\"].split(\"/\")[-1]  # Extract DBpedia title\n",
    "    except Exception as e:\n",
    "        print(f\"Error normalizing {org_name}: {e}\")\n",
    "\n",
    "    return org_name  # Return original if not found\n",
    "\n",
    "# Function to analyze sentiment of a sentence\n",
    "def get_sentiment(sentence):\n",
    "    \"\"\"Analyze sentiment of a sentence using Transformers.\"\"\"\n",
    "    result = sentiment_pipeline(sentence)\n",
    "    sentiment_label = result[0]['label']  # 'POSITIVE' or 'NEGATIVE'\n",
    "    confidence = result[0]['score']\n",
    "\n",
    "    # If confidence is low, classify as NEUTRAL\n",
    "    if confidence < 0.7:\n",
    "        return 'NEUTRAL'\n",
    "    return sentiment_label\n",
    "\n",
    "# Function to extract and save summarized text from PDFs\n",
    "def extract_text_from_pdfs(pdf_dir, output_csv=\"summarized_texts.csv\"):\n",
    "    texts = []\n",
    "    \n",
    "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"filename\", \"summarized_text\"])  # CSV Header\n",
    "\n",
    "        for filename in os.listdir(pdf_dir):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                filepath = os.path.join(pdf_dir, filename)\n",
    "                with fitz.open(filepath) as doc:\n",
    "                    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "                    \n",
    "                    # Summarize the extracted text before further processing\n",
    "                    summarized_text = summarize_text(text)\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    writer.writerow([filename, summarized_text])\n",
    "                    \n",
    "                    # Store in list for further processing\n",
    "                    texts.append((summarized_text, filename))  \n",
    "\n",
    "    return texts  # Return summarized texts if needed\n",
    "\n",
    "\n",
    "# Function to extract organization relationships with sentiment\n",
    "def extract_organization_relationships(text, filename):\n",
    "    \"\"\"Extract organization entities and relationships from text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "\n",
    "    # Create relationships between organizations in the same sentence\n",
    "    for sent in doc.sents:\n",
    "        sent_doc = nlp(sent.text)\n",
    "        entities_in_sent = [normalize_org_name(e.text) for e in sent_doc.ents if e.label_ == \"ORG\"]\n",
    "        entities_in_sent = [e for e in entities_in_sent if e]  # Remove None values\n",
    "\n",
    "        if len(entities_in_sent) >= 2:\n",
    "            sentiment = get_sentiment(sent.text)  # Get sentiment of the sentence\n",
    "            for i in range(len(entities_in_sent) - 1):\n",
    "                relationships.append({\n",
    "                    'source': entities_in_sent[i],\n",
    "                    'target': entities_in_sent[i + 1],\n",
    "                    'sentence': sent.text,\n",
    "                    'sentiment': sentiment,\n",
    "                    'source_file': filename\n",
    "                })\n",
    "\n",
    "    return relationships\n",
    "\n",
    "# Function to save relationships as CSV\n",
    "def save_relationships_to_csv(relationships, output_path):\n",
    "    df = pd.DataFrame(relationships, columns=[\"source\", \"target\", \"sentence\", \"sentiment\", \"source_file\"])\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# Function to visualize organization relationships with sentiment-based colors\n",
    "def visualize_relationships(relationships):\n",
    "    \"\"\"Visualize organization relationships using Pyvis with sentiment-based edge colors.\"\"\"\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", notebook=True, cdn_resources='in_line')\n",
    "\n",
    "    # Add nodes (organizations)\n",
    "    organizations = set()\n",
    "    for relation in relationships:\n",
    "        organizations.add(relation['source'])\n",
    "        organizations.add(relation['target'])\n",
    "\n",
    "    for org in organizations:\n",
    "        net.add_node(org, label=org, color=\"blue\", size=15)\n",
    "\n",
    "    # Add edges with sentiment-based color\n",
    "    for relation in relationships:\n",
    "        sentiment = relation['sentiment']\n",
    "        if sentiment == \"POSITIVE\":\n",
    "            edge_color = \"green\"\n",
    "        elif sentiment == \"NEGATIVE\":\n",
    "            edge_color = \"red\"\n",
    "        else:\n",
    "            edge_color = \"gray\"\n",
    "\n",
    "        net.add_edge(relation['source'], relation['target'], width=2, color=edge_color, title=relation['sentence'])\n",
    "\n",
    "    # Save and show visualization\n",
    "    net.show(\"organization_network.html\")\n",
    "\n",
    "# Main script to process PDFs\n",
    "pdf_dir = \"/Users/benitaleonardi/Downloads/Datathon pdfs\"\n",
    "texts_with_sources = extract_text_from_pdfs(pdf_dir)\n",
    "\n",
    "all_relationships = []\n",
    "\n",
    "for text, filename in texts_with_sources:\n",
    "    relationships = extract_organization_relationships(text, filename)\n",
    "    all_relationships.extend(relationships)\n",
    "\n",
    "# Save relationships to CSV\n",
    "output_csv_path = \"output_relationships.csv\"\n",
    "save_relationships_to_csv(all_relationships, output_csv_path)\n",
    "\n",
    "# Visualize the relationships with sentiment-based edges\n",
    "visualize_relationships(all_relationships)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992b085-409f-464d-b723-84d3d7cfc7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62db8988-caf1-4db7-bdec-c0a10083a4ab",
   "metadata": {},
   "source": [
    "### USE THIS INSTEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e29904-e7f6-4824-9c6b-7c126416e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country_network.html\n",
      "✅ Graph saved as country_network.html\n",
      "organization_network.html\n",
      "✅ Graph saved as organization_network.html\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Hugging Face sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# OpenAI API Key (Set this securely in your environment)\n",
    "client = openai.OpenAI(api_key=\"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\") \n",
    "\n",
    "# Function to summarize text using OpenAI GPT-4o-mini\n",
    "def summarize_text(text):\n",
    "    \"\"\"Summarize text and extract key organization relationships using OpenAI GPT-4o-mini.\"\"\"\n",
    "    prompt = \"\"\"Identify the main points in the article provided.\n",
    "    Given these main points, find relationships involving entities of type Organization.\n",
    "    \\n\\nArticle:\\n\"\"\" + text[:4000]  # Truncate to avoid API limits\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            store=True,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        summary = completion.choices[0].message.content\n",
    "        return summary.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return text  # Fallback to original text if API fails\n",
    "\n",
    "# Function to normalize organization names using DBpedia Spotlight\n",
    "def normalize_org_name(org_name):\n",
    "    \"\"\"Normalize organization names using DBpedia Spotlight API.\"\"\"\n",
    "    url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    params = {\"text\": org_name, \"confidence\": 0.5}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"Resources\" in data:\n",
    "                return data[\"Resources\"][0][\"@URI\"].split(\"/\")[-1]  # Extract DBpedia title\n",
    "    except Exception as e:\n",
    "        print(f\"Error normalizing {org_name}: {e}\")\n",
    "\n",
    "    return org_name  # Return original if not found\n",
    "\n",
    "# Function to analyze sentiment of a sentence\n",
    "def get_sentiment(sentence):\n",
    "    \"\"\"Analyze sentiment of a sentence using Transformers.\"\"\"\n",
    "    result = sentiment_pipeline(sentence)\n",
    "    sentiment_label = result[0]['label']  # 'POSITIVE' or 'NEGATIVE'\n",
    "    confidence = result[0]['score']\n",
    "\n",
    "    # If confidence is low, classify as NEUTRAL\n",
    "    if confidence < 0.7:\n",
    "        return 'NEUTRAL'\n",
    "    return sentiment_label\n",
    "\n",
    "# Function to extract and save summarized text from PDFs\n",
    "def extract_text_from_pdfs(pdf_dir, output_csv=\"summarized_texts.csv\"):\n",
    "    \"\"\"Extract text from PDFs, summarize using GPT, and save to CSV.\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"filename\", \"summarized_text\"])  # CSV Header\n",
    "\n",
    "        for filename in os.listdir(pdf_dir):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                filepath = os.path.join(pdf_dir, filename)\n",
    "                with fitz.open(filepath) as doc:\n",
    "                    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "                    \n",
    "                    # Summarize the extracted text before further processing\n",
    "                    summarized_text = summarize_text(text)\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    writer.writerow([filename, summarized_text])\n",
    "                    \n",
    "                    # Store in list for further processing\n",
    "                    texts.append((summarized_text, filename))  \n",
    "\n",
    "    return texts  # Return summarized texts if needed\n",
    "\n",
    "# Function to extract relationships from text\n",
    "def extract_relationships(text, filename, entity_label):\n",
    "    \"\"\"Extract relationships for a specific entity type (Country or Organization).\"\"\"\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_doc = nlp(sent.text)\n",
    "        entities_in_sent = [e.text for e in sent_doc.ents if e.label_ == entity_label]\n",
    "        entities_in_sent = list(set(entities_in_sent))  # Remove duplicates\n",
    "\n",
    "        if len(entities_in_sent) >= 2:\n",
    "            sentiment = get_sentiment(sent.text)\n",
    "            for i in range(len(entities_in_sent) - 1):\n",
    "                relationships.append({\n",
    "                    'source': entities_in_sent[i],\n",
    "                    'target': entities_in_sent[i + 1],\n",
    "                    'sentence': sent.text,\n",
    "                    'sentiment': sentiment,\n",
    "                    'source_file': filename\n",
    "                })\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# Function to save relationships as CSV\n",
    "def save_relationships_to_csv(relationships, output_path):\n",
    "    df = pd.DataFrame(relationships, columns=[\"source\", \"target\", \"sentence\", \"sentiment\", \"source_file\"])\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# Function to visualize relationships with filtering\n",
    "def visualize_filtered_relationships(relationships, entity_type, output_file=\"network.html\"):\n",
    "    \"\"\"Generate an interactive Pyvis graph with filtering for top entities.\"\"\"\n",
    "    \n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", notebook=True, cdn_resources='in_line')\n",
    "    net.force_atlas_2based(gravity=-30, central_gravity=0.02, spring_length=250, spring_strength=0.1)\n",
    "    \n",
    "    # Step 1: Count entity mentions\n",
    "    entity_mentions = defaultdict(int)\n",
    "    for rel in relationships:\n",
    "        entity_mentions[rel[\"source\"]] += 1\n",
    "        entity_mentions[rel[\"target\"]] += 1\n",
    "    \n",
    "    # Step 2: Keep only the top 30 most mentioned entities\n",
    "    N = 30\n",
    "    top_entities = sorted(entity_mentions.items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "    top_entities = {entity for entity, _ in top_entities}\n",
    "\n",
    "    # Step 3: Filter relationships (keep only strong ones)\n",
    "    strong_relationships = defaultdict(int)\n",
    "    for r in relationships:\n",
    "        strong_relationships[(r[\"source\"], r[\"target\"])] += 1\n",
    "    \n",
    "    filtered_relationships = [\n",
    "        r for r in relationships if strong_relationships[(r[\"source\"], r[\"target\"])] >= 4\n",
    "        and r[\"source\"] in top_entities and r[\"target\"] in top_entities\n",
    "    ]\n",
    "\n",
    "    # Step 4: Add nodes\n",
    "    for entity in top_entities:\n",
    "        color = \"blue\" if entity_type == \"country\" else \"cyan\"\n",
    "        net.add_node(entity, label=entity, color=color, size=15)\n",
    "\n",
    "    # Step 5: Add edges\n",
    "    for relation in filtered_relationships:\n",
    "        edge_color = \"green\" if relation[\"sentiment\"] == \"POSITIVE\" else \"red\" if relation[\"sentiment\"] == \"NEGATIVE\" else \"gray\"\n",
    "        net.add_edge(relation[\"source\"], relation[\"target\"], width=2, color=edge_color, title=relation[\"sentence\"])\n",
    "\n",
    "    # Save and show the graph\n",
    "    net.show(output_file)\n",
    "    print(f\"✅ Graph saved as {output_file}\")\n",
    "\n",
    "# Main script to process PDFs\n",
    "pdf_dir = \"/Users/benitaleonardi/Downloads/Datathon pdfs\"\n",
    "texts_with_sources = extract_text_from_pdfs(pdf_dir)\n",
    "\n",
    "country_relationships = []\n",
    "organization_relationships = []\n",
    "\n",
    "for text, filename in texts_with_sources:\n",
    "    country_relationships.extend(extract_relationships(text, filename, \"GPE\"))\n",
    "    organization_relationships.extend(extract_relationships(text, filename, \"ORG\"))\n",
    "\n",
    "# Save relationships to separate CSV files\n",
    "save_relationships_to_csv(country_relationships, \"country_relationships.csv\")\n",
    "save_relationships_to_csv(organization_relationships, \"organization_relationships.csv\")\n",
    "\n",
    "# Visualize\n",
    "visualize_filtered_relationships(country_relationships, \"country\", \"country_network.html\")\n",
    "visualize_filtered_relationships(organization_relationships, \"organization\", \"organization_network.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753861bc-7a5c-4b61-b3b5-af749e4ce1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
